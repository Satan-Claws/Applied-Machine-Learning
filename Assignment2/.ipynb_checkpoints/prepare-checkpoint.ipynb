{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e7683f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading file: 100%|██████████| 5574/5574 [00:03<00:00, 1786.78line/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data saved to data/raw_data.csv\n",
      "First version splits saved with random_state=42\n",
      "Train size: 2787, Validation size: 1393, Test size: 1394\n",
      "\n",
      "First Version - Distribution of Target Variable:\n",
      "Train: {0: 2404, 1: 383}\n",
      "Validation: {0: 1212, 1: 181}\n",
      "Test: {0: 1211, 1: 183}\n",
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
      "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
      "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/raw_data.csv |0.00 [00:00,     ?fi\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/shoru/Desktop/AML_git/Applied-Machine-Learning/As\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/raw_data.csv to cache     0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/shoru/Desktop/AML_0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|1/1 [00:00, 113.10file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/raw_data.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/train.csv |0.00 [00:00,     ?file/\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/shoru/Desktop/AML_git/Applied-Machine-Learning/As\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/train.csv to cache        0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/shoru/Desktop/AML_0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|1/1 [00:00, 130.75file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/train.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/validation.csv |0.00 [00:00,     ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/shoru/Desktop/AML_git/Applied-Machine-Learning/As\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/validation.csv to cache   0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/shoru/Desktop/AML_0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|1/1 [00:00, 122.10file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/validation.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/test.csv |0.00 [00:00,     ?file/s\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/shoru/Desktop/AML_git/Applied-Machine-Learning/As\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/test.csv to cache         0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/shoru/Desktop/AML_0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|1/1 [00:00, 125.87file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/test.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m[main 25cf17c] First data split with seed 42\n",
      " 7 files changed, 16 insertions(+), 6 deletions(-)\n",
      " create mode 100644 Assignment2/.dvc/.gitignore\n",
      " create mode 100644 Assignment2/.dvc/config\n",
      " create mode 100644 Assignment2/.dvcignore\n",
      " create mode 100644 Assignment2/data/.gitignore\n",
      "First version commit hash: 25cf17c01b4c39e202837b4179e61f23f1ff39c2\n",
      "\n",
      "Second version splits saved with random_state=123\n",
      "Train size: 2787, Validation size: 1393, Test size: 1394\n",
      "\n",
      "Second Version - Distribution of Target Variable:\n",
      "Train: {0: 2413, 1: 374}\n",
      "Validation: {0: 1208, 1: 185}\n",
      "Test: {0: 1206, 1: 188}\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/train.csv |0.00 [00:00,     ?file/\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/shoru/Desktop/AML_git/Applied-Machine-Learning/As\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/train.csv to cache        0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/shoru/Desktop/AML_0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|1/1 [00:00, 127.65file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/train.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/validation.csv |0.00 [00:00,     ?\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/shoru/Desktop/AML_git/Applied-Machine-Learning/As\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/validation.csv to cache   0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/shoru/Desktop/AML_0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|1/1 [00:00, 130.10file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/validation.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                       core\u001b[39m>\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in data/test.csv |0.00 [00:00,     ?file/s\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/shoru/Desktop/AML_git/Applied-Machine-Learning/As\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Adding data/test.csv to cache         0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/shoru/Desktop/AML_0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|1/1 [00:00, 130.53file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/test.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m[main e6132ec] Updated data split with seed 123\n",
      " 3 files changed, 6 insertions(+), 6 deletions(-)\n",
      "Second version commit hash: e6132ec5ccfdae0242236843cda0e25991aaac88\n",
      "\n",
      "=== Checking out first version ===\n",
      "Building workspace index                              |5.00 [00:00,  892entry/s]\n",
      "Comparing indexes                                    |6.00 [00:00, 8.29kentry/s]\n",
      "Applying changes                                      |3.00 [00:00, 3.87kfile/s]\n",
      "\u001b[33mM\u001b[0m       data/test.csv\n",
      "\u001b[33mM\u001b[0m       data/train.csv\n",
      "\u001b[33mM\u001b[0m       data/validation.csv\n",
      "\u001b[0m\n",
      "Distribution for first version (seed 42):\n",
      "Train: {0: 2404, 1: 383}\n",
      "Validation: {0: 1212, 1: 181}\n",
      "Test: {0: 1211, 1: 183}\n",
      "\n",
      "=== Checking out second version ===\n",
      "Building workspace index                             |5.00 [00:00, 1.40kentry/s]\n",
      "Comparing indexes                                    |6.00 [00:00, 9.74kentry/s]\n",
      "Applying changes                                      |3.00 [00:00, 4.68kfile/s]\n",
      "\u001b[33mM\u001b[0m       data/test.csv\n",
      "\u001b[33mM\u001b[0m       data/train.csv\n",
      "\u001b[33mM\u001b[0m       data/validation.csv\n",
      "\u001b[0m\n",
      "Distribution for second version (seed 123):\n",
      "Train: {0: 2413, 1: 374}\n",
      "Validation: {0: 1208, 1: 185}\n",
      "Test: {0: 1206, 1: 188}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "file_path = \"sms+spam+collection/SMSSpamCollection\"\n",
    "\n",
    "ham_messages = []\n",
    "spam_messages = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in tqdm(lines, desc=\"reading file\", unit=\"line\"):\n",
    "    if line.startswith(\"ham\"):\n",
    "        ham_messages.append(preprocess_text(line[4:].strip()))\n",
    "    elif line.startswith(\"spam\"):\n",
    "        spam_messages.append(preprocess_text(line[5:].strip()))\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"text\": ham_messages + spam_messages,\n",
    "    \"label\": [0] * len(ham_messages) + [1] * len(spam_messages)  # 1 for spam, 0 for ham\n",
    "})\n",
    "\n",
    "# Save raw data\n",
    "data.to_csv(\"data/raw_data.csv\", index=False)\n",
    "print(\"Raw data saved to data/raw_data.csv\")\n",
    "\n",
    "train, temp_df = train_test_split(data, test_size=0.5, random_state=42)\n",
    "validation, test = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "validation.to_csv(\"data/validation.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "print(\"First version splits saved with random_state=42\")\n",
    "print(f\"Train size: {len(train)}, Validation size: {len(validation)}, Test size: {len(test)}\")\n",
    "\n",
    "print(\"\\nFirst Version - Distribution of Target Variable:\")\n",
    "print(\"Train:\", train['label'].value_counts().to_dict())\n",
    "print(\"Validation:\", validation['label'].value_counts().to_dict())\n",
    "print(\"Test:\", test['label'].value_counts().to_dict())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab6918-adcb-43f8-b386-8964b0f8e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DVC with --subdir flag since we're in a subdirectory of a Git repo\n",
    "!dvc init --subdir\n",
    "\n",
    "# Track the data with DVC\n",
    "!dvc add data/raw_data.csv\n",
    "!dvc add data/train.csv\n",
    "!dvc add data/validation.csv\n",
    "!dvc add data/test.csv\n",
    "\n",
    "# Commit this version\n",
    "!git add data/.gitignore data/*.dvc .dvc\n",
    "!git commit -m \"First data split with seed 42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "commit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()\n",
    "with open('first_version_commit.txt', 'w') as f:\n",
    "    f.write(commit_hash)\n",
    "print(f\"First version commit hash: {commit_hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda0b00-6784-4384-ab3e-88eeabc5813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, temp_df = train_test_split(data, test_size=0.5, random_state=123)\n",
    "validation, test = train_test_split(temp_df, test_size=0.5, random_state=123)\n",
    "\n",
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "validation.to_csv(\"data/validation.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "print(\"\\nSecond version splits saved with random_state=123\")\n",
    "print(f\"Train size: {len(train)}, Validation size: {len(validation)}, Test size: {len(test)}\")\n",
    "\n",
    "print(\"\\nSecond Version - Distribution of Target Variable:\")\n",
    "print(\"Train:\", train['label'].value_counts().to_dict())\n",
    "print(\"Validation:\", validation['label'].value_counts().to_dict())\n",
    "print(\"Test:\", test['label'].value_counts().to_dict())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac7fd5-2c0a-4a98-9cfe-4c44715fb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc add data/train.csv\n",
    "!dvc add data/validation.csv\n",
    "!dvc add data/test.csv\n",
    "\n",
    "!git add data/*.dvc\n",
    "!git commit -m \"Updated data split with seed 123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5481f-5048-40ae-be65-222ee8855061",
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()\n",
    "with open('second_version_commit.txt', 'w') as f:\n",
    "    f.write(commit_hash)\n",
    "print(f\"Second version commit hash: {commit_hash}\")\n",
    "\n",
    "print(\"\\n=== Checking out first version ===\")\n",
    "!git checkout $(cat first_version_commit.txt) -- data/*.dvc\n",
    "!dvc checkout\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "validation = pd.read_csv('data/validation.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "\n",
    "print(\"\\nDistribution for first version (seed 42):\")\n",
    "print(\"Train:\", train['label'].value_counts().to_dict())\n",
    "print(\"Validation:\", validation['label'].value_counts().to_dict())\n",
    "print(\"Test:\", test['label'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abf97fe-586f-472f-a1ea-e59f4710a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Checking out second version ===\")\n",
    "!git checkout $(cat second_version_commit.txt) -- data/*.dvc\n",
    "!dvc checkout\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "validation = pd.read_csv('data/validation.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(\"\\nDistribution for second version (seed 123):\")\n",
    "print(\"Train:\", train['label'].value_counts().to_dict())\n",
    "print(\"Validation:\", validation['label'].value_counts().to_dict())\n",
    "print(\"Test:\", test['label'].value_counts().to_dict())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "vectorizer.fit(data['text'])\n",
    "import pickle\n",
    "with open('data/vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
