{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8a23f0-6bea-4454-b77a-3f1b8ab75653",
   "metadata": {},
   "source": [
    "## import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8c4198-dda2-427c-b456-30434bb08473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fa921-d427-4ae1-b648-02d535b741f0",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c89c72-3a5e-4654-a860-5a82729df82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"validation.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_train, y_train = train_df['text'], train_df['label']\n",
    "X_val, y_val = val_df['text'], val_df['label']\n",
    "X_test, y_test = test_df['text'], test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7f043-408d-4e5a-a1a1-0eb7d1d0a9ca",
   "metadata": {},
   "source": [
    "## Convert words to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3a58d2-5ea2-4e10-90b1-fedb7bf9ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68730025-5c92-4937-8807-267d7c610fc8",
   "metadata": {},
   "source": [
    "## Functions required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e08a26-a4b6-4de9-8d94-19d9fccd2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def score_model(model, X, y):\n",
    "    return model.score(X, y)\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y, y_pred),\n",
    "        \"precision\": precision_score(y, y_pred, pos_label=1),\n",
    "        \"recall\": recall_score(y, y_pred, pos_label=1),\n",
    "        \"f1_score\": f1_score(y, y_pred, pos_label=1)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def validate_model(model, X_train, y_train, X_val, y_val):\n",
    "    model = fit_model(model, X_train, y_train)\n",
    "    \n",
    "    print(\"Train Scores:\")\n",
    "    train_metrics = evaluate_model(model, X_train, y_train)\n",
    "    print(train_metrics)\n",
    "\n",
    "    print(\"Validation Scores:\")\n",
    "    val_metrics = evaluate_model(model, X_val, y_val)\n",
    "    print(val_metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56a2e9-e2a4-4b5b-8613-97502defbc6f",
   "metadata": {},
   "source": [
    "## Running on all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ae5fb5-417d-4f48-ad5a-d1d6dd8f548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Score For: Naive Bayes----\n",
      "Train Scores:\n",
      "{'accuracy': 0.9773950484391819, 'precision': 1.0, 'recall': 0.8405063291139241, 'f1_score': 0.9133425034387895}\n",
      "Validation Scores:\n",
      "{'accuracy': 0.9676956209619526, 'precision': 1.0, 'recall': 0.7413793103448276, 'f1_score': 0.8514851485148515}\n",
      "\n",
      "---- Score For: Logistic Regression----\n",
      "Train Scores:\n",
      "{'accuracy': 0.9605310369573018, 'precision': 0.9863481228668942, 'recall': 0.7316455696202532, 'f1_score': 0.8401162790697675}\n",
      "Validation Scores:\n",
      "{'accuracy': 0.9619526202440776, 'precision': 0.935251798561151, 'recall': 0.7471264367816092, 'f1_score': 0.8306709265175719}\n",
      "\n",
      "---- Score For: Random Forest----\n",
      "Train Scores:\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}\n",
      "Validation Scores:\n",
      "{'accuracy': 0.9791816223977028, 'precision': 0.9865771812080537, 'recall': 0.8448275862068966, 'f1_score': 0.9102167182662538}\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    print(\"\\n---- Score For: \"+ str(name)+ \"----\")\n",
    "    trained_models[name] = validate_model(model, X_train_vec, y_train, X_val_vec, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa324ca-f206-4a71-8255-dee0d43a0179",
   "metadata": {},
   "source": [
    "## Picking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53c9812-428d-4f26-b0bc-e5ba322be134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes's Score: 0.9713055954088953\n",
      "Logistic Regression's Score: 0.9655667144906743\n",
      "Random Forest's Score: 0.9770444763271162\n",
      "The Best model out of the 3:  RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "test_scores = {}\n",
    "for name, model in trained_models.items():\n",
    "    curr_score = score_model(model, X_test_vec, y_test)\n",
    "    print(str(name)+\"'s Score:\", curr_score)\n",
    "    test_scores[name]=curr_score\n",
    "    \n",
    "best_model_name = max(test_scores, key=test_scores.get)\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "\n",
    "print(\"The Best model out of the 3: \", best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9853e-0fb0-409e-901e-c9433104a10a",
   "metadata": {},
   "source": [
    "## Testing on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8553abd9-d813-42a1-a157-57ad61cc558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Wrong phone ! phone ! answer one assume people n't well\n",
      "Actual Label: Not Spam, Predicted Label: Not Spam\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Haha , first person gon na ask\n",
      "Actual Label: Not Spam, Predicted Label: Not Spam\n",
      "--------------------------------------------------------------------------------\n",
      "Text: come people\n",
      "Actual Label: Not Spam, Predicted Label: Not Spam\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Yup ok ...\n",
      "Actual Label: Not Spam, Predicted Label: Not Spam\n",
      "--------------------------------------------------------------------------------\n",
      "Text: â€˜ leave around four , ok ?\n",
      "Actual Label: Not Spam, Predicted Label: Not Spam\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "names_of_label={0:\"Not Spam\", 1: \"Spam\"}\n",
    "\n",
    "num_samples = 5 \n",
    "sample_indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    print(f\"Text: {X_test.iloc[idx]}\")\n",
    "    print(f\"Actual Label: {names_of_label[y_test.iloc[idx]]}, Predicted Label: {names_of_label[y_test_pred[idx]]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e312e-8583-41f8-82f2-93fe425727b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
